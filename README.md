# TextFlow

**TextFlow** is a character-level language model that learns to generate human-like names from raw text. The project demonstrates a progressive deep learning pipeline that evolves from simple statistical models to sophisticated neural architectures like WaveNet.

---

## ✦ Project Objective

The goal of TextFlow is to explore how model complexity impacts the quality of text generation. By starting from a basic bigram frequency model and scaling up to a WaveNet-inspired deep neural network, the project highlights improvements in:

- Prediction accuracy
- Loss reduction
- Generation realism

This journey from simple to advanced models gives an in-depth view of how architectures learn textual patterns over time.

---

## ✦ What Has Been Implemented

The project is divided into four stages:

### 1. Bigram Language Model
- Character pair frequency model
- No learning parameters
- Generates names purely based on transition probabilities

### 2. MLP (Multi-Layer Perceptron)
- Fully connected network trained on n-grams
- Learns embeddings and basic sequence dependencies
- Implemented from scratch in PyTorch

### 3. Intermediate MLP Optimizations
- Hyperparameter tuning
- Manual batching and weight decay experiments
- Better generalization and reduced overfitting

### 4. WaveNet-Inspired Model
- Uses a stack of dilated 1D convolutional layers
- Captures long-range dependencies in character sequences
- Fully vectorized PyTorch implementation
- Best performance in both loss and quality of name generation

---

## ✦ Sample Names Generated by the WaveNet Model

|         |         |         |         |              |
|---------|---------|---------|---------|--------------|
| jahya   | usten   | lorrison| adrianna| franciea     |
| kaylen  | mccenne | bawson  | briyon  | sylonna      |
| jaice   | myndal  | dayphos | petric  | isabellamaris|
| hulayah | sirence | ayen    | edulija | rohan        |


These names were generated after training on a curated dataset of real human names. Notice how the model not only mimics realistic phonetics but also forms longer and more complex structures like `isabellamaris` and `edulija`.

---

## ✦ Loss Curve

![Loss Over Epochs](assets/loss-curve.png)


---

## ✦ Loss Comparison Across Models

| Model               | Final Training Loss |
|---------------------|---------------------|
| Bigram              | 2.48                |
| MLP                 | 2.07                |
| WaveNet             | 1.83                |
| Initial (Untrained) | 3.25                |

As evident, the progression from simple to deep architectures led to consistent improvements in training loss.

---

## ✦ Folder Structure
<!-- TREEVIEW START -->
```bash
text-flow/
├── data/ # Dataset of names (input)
├── notebooks/ # Jupyter experiments and model development
├── models/ # PyTorch model definitions (MLP, WaveNet)
└── README.md
```
<!-- TREEVIEW END -->
---